{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Indeed_webscrape.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"N3LuHeIHj6q2","colab_type":"code","outputId":"9ed2fe7f-496b-4ddf-f76c-cf787fc4ebb6","executionInfo":{"status":"ok","timestamp":1565880299231,"user_tz":240,"elapsed":7873,"user":{"displayName":"Brandon Lewis","photoUrl":"https://lh4.googleusercontent.com/-COjqtyP8ljk/AAAAAAAAAAI/AAAAAAAAAAc/oiRopWs_yuA/s64/photo.jpg","userId":"14262464945113289318"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["import requests\n","import bs4\n","from bs4 import BeautifulSoup\n","!pip install selenium\n","!pip install ChromeDriver\n","from selenium import webdriver\n","from selenium.webdriver import ActionChains\n","from selenium.webdriver.common.keys import Keys\n","import pandas as pd\n","import time"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: selenium in /usr/local/lib/python3.6/dist-packages (3.141.0)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n","Collecting ChromeDriver\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/5e/1daf3c71852f5b8536e30b2afa1e6442c454e91947678ac1a37daba2d7f5/chromedriver-2.24.1-py2.py3-none-any.whl (14.3MB)\n","\u001b[K     |████████████████████████████████| 14.3MB 3.7MB/s \n","\u001b[?25hInstalling collected packages: ChromeDriver\n","Successfully installed ChromeDriver-2.24.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nDei3Ks6rAxV","colab_type":"code","colab":{}},"source":["URL = 'https://www.indeed.com/jobs?q=data%20scientist&l=Atlanta%2C%20GA&advn=5432824348636674'\n","#conducting a request of the stated URL above:\n","page = requests.get(URL)\n","#specifying a desired format of “page” using the html parser - this allows python to read the various components of the page, rather than treating it as one long string.\n","soup = BeautifulSoup(page.content, 'html.parser')\n","#printing soup in a more structured tree format that makes for easier reading\n","print(soup.prettify())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z0tZjiSRMdC-","colab_type":"code","colab":{}},"source":["def get_soup(url):\n","    driver = webdriver.Chrome()\n","    driver.get(url)\n","    counter = 0\n","    while True:\n","        try:\n","            click_me = driver.find_element_by_xpath('/html/body/main/section[1]/button')\n","            time.sleep(2)\n","            click_me.click()\n","            counter += 1\n","            if counter == 40:\n","                break\n","        except Exception as e:\n","            break\n","    my_var = driver.find_elements_by_class_name('result-card__full-card-link')\n","    html = driver.page_source\n","    soup = BeautifulSoup(html, 'html.parser')\n","    driver.close()\n","    return soup"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b_8T22BANPRC","colab_type":"code","colab":{}},"source":["def grab_job_links(soup):\n","    urls = []\n","    for link in soup.find_all('span', {'class':'company'}):\n","      partial_url = link.a.get('href')\n","      url = 'https://www.indeed.com' + partial_url\n","      urls.append(url)\n","      time.sleep(1)\n","    return urls"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DUwyFj4ufO7Q","colab_type":"code","colab":{}},"source":["def get_urls(position, num_pages, location):\n","    \"\"\"\n","    Get all the job posting URLs resulted from a specific search.\n","    \n","    Parameters:\n","        position: job title to query\n","        num_pages: number of pages needed\n","        location: city to search in\n","    \n","    Returns:\n","        urls: a list of job posting URL's (when num_pages valid)\n","        max_pages: maximum number of pages allowed ((when num_pages invalid))\n","    \"\"\"\n","    # We always need the first page\n","#     base_url = f'https://www.indeed.com/jobs?q={job}&l={city}&start='\n","#     soup = get_soup(base_url)\n","#     urls = grab_job_links(soup)\n","    \n","#     driver = webdriver.Chrome()\n","#     driver.get(url)\n","    base_url=\"https://www.indeed.com/jobs?q=\"\n","    position=position.replace(\" \", \"%20\").strip()\n","    location=location.replace(\" \", \"%20\").strip()\n","    url = f'https://www.indeed.com/jobs?q={position}&l={location}%2C%20GA&advn=5432824348636674&vjk=9f2697c604f96ef1'\n","    soup = get_soup(url)\n","    \n","    \n","    \n","    # Get the total number of postings found \n","    posting_count_string = soup.find(name='div', attrs={'id':\"searchCount\"}).get_text()\n","    posting_count_string = posting_count_string[posting_count_string.find('of')+2:].strip()\n","    #print('posting_count_string: {}'.format(posting_count_string))\n","    #print('type is: {}'.format(type(posting_count_string)))\n","    \n","    try:\n","        posting_count = int(posting_count_string)\n","    except ValueError: # deal with special case when parsed string is \"360 jobs\"\n","        posting_count = int(re.search('\\d+', posting_count_string).group(0))\n","        #print('posting_count: {}'.format(posting_count))\n","        #print('\\ntype: {}'.format(type(posting_count)))\n","    finally:\n","        posting_count = 330 # setting to 330 when unable to get the total\n","        pass\n","    \n","    # Limit nunmber of pages to get\n","    max_pages = round(posting_count / 10) - 3\n","    if num_pages > max_pages:\n","        print('returning max_pages!!')\n","        return max_pages\n","    \n","        # Additional work is needed when more than 1 page is requested\n","    if num_pages >= 2:\n","        # Start loop from page 2 since page 1 has been dealt with above\n","        for i in range(2, num_pages+1):\n","            num = (i-1) * 10\n","            base_url = f'https://www.indeed.com/jobs?q={job}&l={city}&start={num}'\n","            try:\n","                soup = get_soup(base_url)\n","                # We always combine the results back to the list\n","                urls += grab_job_links(soup)\n","            except:\n","                continue\n","\n","    # Check to ensure the number of urls gotten is correct\n","    #assert len(urls) == num_pages * 10, \"There are missing job links, check code!\"\n","\n","    return urls     "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1_s_R-MDRO_n","colab_type":"code","colab":{}},"source":["def get_posting(urls):    \n","    position_list = []\n","    company_list = []\n","    location_list = []\n","    description_list = []\n","    \n","    for url in urls:\n","      soup = get_soup(url)\n","      try:\n","        position = soup.find(name='div', attrs={'class':'vjs-jobtitle'})\n","        position_list.append(position.lower())\n","                \n","        company = soup.find(name='div', attrs={'class':'vjs-cn'}).get_text()\n","        company_list.append(company.lower())\n","                \n","        location = soup.find(name='div', attrs={'class':'vjs-loc'}).get_text()\n","        location_list.append(location.lower())\n","                \n","        decription = soup.find(name='div', attrs={'class':'vjs-content'}).get_text()\n","        description_list.append(description.lower())\n","      except AttributeError:\n","        position_list.pop()\n","        continue\n","                \n","      df = pd.DataFrame({'position': position_list,\n","                         'company': company_list,\n","                         'location': location_list,\n","                         'description': description})\n","    return df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oE1PCpCFpuUU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":443},"outputId":"7a9a3edf-a992-4bb8-e069-188b39eef0db","executionInfo":{"status":"error","timestamp":1565880307552,"user_tz":240,"elapsed":322,"user":{"displayName":"Brandon Lewis","photoUrl":"https://lh4.googleusercontent.com/-COjqtyP8ljk/AAAAAAAAAAI/AAAAAAAAAAc/oiRopWs_yuA/s64/photo.jpg","userId":"14262464945113289318"}}},"source":["locations = ['New+York','Chicago','San+Francisco'] #, 'Austin', 'Seattle', 'Los+Angeles', 'Philadelphia', 'Atlanta', 'Kansas+City', 'Dallas', 'Pittsburgh', 'Portland', 'Phoenix', 'Denver', 'Houston', 'Miami', 'Washington+DC', 'Boulder', 'Detroit', 'Minneapolis', 'Charlotte']\n","positions = ['data scientist', 'data engineer', 'data analyst', 'machine learning']\n","\n","for location in locations:\n","  for position in positions:\n","    print(f'Starting {location} {position}')\n","    get_urls(position, 10, location)\n","    df = get_posting(urls)\n","    df.to_csv(f'df_{location}_{position}.csv', index = False)\n","    print(f'{location} {pos} complete')"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Starting New+York data scientist\n"],"name":"stdout"},{"output_type":"error","ename":"WebDriverException","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/selenium/webdriver/common/service.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m                                             \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                                             stdin=PIPE)\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    728\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1363\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'chromedriver': 'chromedriver'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-4619b65e92c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Starting {location} {position}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mget_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_posting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'df_{location}_{position}.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-824c83e93eb4>\u001b[0m in \u001b[0;36mget_urls\u001b[0;34m(position, num_pages, location)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%20\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'https://www.indeed.com/jobs?q={position}&l={location}%2C%20GA&advn=5432824348636674&vjk=9f2697c604f96ef1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-bc74b96b0348>\u001b[0m in \u001b[0;36mget_soup\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/selenium/webdriver/chrome/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, keep_alive)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mservice_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             log_path=service_log_path)\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/selenium/webdriver/common/service.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 raise WebDriverException(\n\u001b[1;32m     82\u001b[0m                     \"'%s' executable needs to be in PATH. %s\" % (\n\u001b[0;32m---> 83\u001b[0;31m                         os.path.basename(self.path), self.start_error_message)\n\u001b[0m\u001b[1;32m     84\u001b[0m                 )\n\u001b[1;32m     85\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEACCES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mWebDriverException\u001b[0m: Message: 'chromedriver' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home\n"]}]},{"cell_type":"code","metadata":{"id":"AYO53Wxeq7IE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}