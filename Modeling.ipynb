{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the difference between a Data Scientist and a Data Analyst?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our online and in-person data scientist programs are maturing there is interest from both the Product and Curriculum teams on having data-driven insights on the data science job market to make sure that the Flatiron school's offerings and marketing best fit the evolving data science job market our graduates are heading into. \n",
    "\n",
    "The first step in this exploration is to try and get more clear definitions as to the difference between a data scientist and a data analyst. It is well known in the data science community that the understanding of the field among the general market, hiring managers, and HR recruiters is quite variable and there is significant overlap between roles that have widely varying titles. To try and get some clarity we have scraped a dataset from LinkedIn covering data scientist and data analyst roles and filtered it for roles in NYC, Atlanta, and Kansas City, MO aiming to incorporate listings from a major tech hub, a developing tech hub, and a non-technical job market so as to be representative of the variety of job markets our graduates will be entering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h-vQjvXDzM2t"
   },
   "outputs": [],
   "source": [
    "#library imports - obtain, scrub, explore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0VHZ72Gky1Zm"
   },
   "source": [
    "OBTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 305,
     "status": "error",
     "timestamp": 1565949522705,
     "user": {
      "displayName": "Brandon Lewis",
      "photoUrl": "https://lh4.googleusercontent.com/-COjqtyP8ljk/AAAAAAAAAAI/AAAAAAAAAAc/oiRopWs_yuA/s64/photo.jpg",
      "userId": "14262464945113289318"
     },
     "user_tz": 240
    },
    "id": "_plxOEmVyuSB",
    "outputId": "cfaacc31-6aea-4115-8c90-a59c8de562b0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>description</th>\n",
       "      <th>job_cat</th>\n",
       "      <th>loc</th>\n",
       "      <th>location</th>\n",
       "      <th>position</th>\n",
       "      <th>position_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Loftium</td>\n",
       "      <td>About the role\\nMachine learning is core to ou...</td>\n",
       "      <td>machine learning</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle, Washington, United States</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zume Inc.</td>\n",
       "      <td>Who We Are\\n\\nZume is on a quest to be the mos...</td>\n",
       "      <td>machine learning</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle, WA, US</td>\n",
       "      <td>Machine Learning Engineer - Platform</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRC Staffing Services, Inc.</td>\n",
       "      <td>The goal is to lead the processes from infrast...</td>\n",
       "      <td>machine learning</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle, WA, US</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bidco</td>\n",
       "      <td>We are looking for a Machine Learning Engineer...</td>\n",
       "      <td>machine learning</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle, WA, US</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Compass /</td>\n",
       "      <td>Engineering\\n\\nMachine Learning Engineer\\n\\nSe...</td>\n",
       "      <td>machine learning</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle, WA, US</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       company  \\\n",
       "0                      Loftium   \n",
       "1                    Zume Inc.   \n",
       "2  TRC Staffing Services, Inc.   \n",
       "3                        bidco   \n",
       "4                    Compass /   \n",
       "\n",
       "                                         description           job_cat  \\\n",
       "0  About the role\\nMachine learning is core to ou...  machine learning   \n",
       "1  Who We Are\\n\\nZume is on a quest to be the mos...  machine learning   \n",
       "2  The goal is to lead the processes from infrast...  machine learning   \n",
       "3  We are looking for a Machine Learning Engineer...  machine learning   \n",
       "4  Engineering\\n\\nMachine Learning Engineer\\n\\nSe...  machine learning   \n",
       "\n",
       "           loc                            location  \\\n",
       "0  Seattle, WA  Seattle, Washington, United States   \n",
       "1  Seattle, WA                     Seattle, WA, US   \n",
       "2  Seattle, WA                     Seattle, WA, US   \n",
       "3  Seattle, WA                     Seattle, WA, US   \n",
       "4  Seattle, WA                     Seattle, WA, US   \n",
       "\n",
       "                               position position_low  \n",
       "0             Machine Learning Engineer          NaN  \n",
       "1  Machine Learning Engineer - Platform          NaN  \n",
       "2             Machine Learning Engineer          NaN  \n",
       "3             Machine Learning Engineer          NaN  \n",
       "4             Machine Learning Engineer          NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/jobs.csv')\n",
    "df = pd.DataFrame(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['location','position_low'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>company</th>\n",
       "      <th>description</th>\n",
       "      <th>job_cat</th>\n",
       "      <th>loc</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2078</td>\n",
       "      <td>PRI Technology</td>\n",
       "      <td>Sr. Data Scientist\\n\\nThe Sr. Data Scientist i...</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Sr. Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2079</td>\n",
       "      <td>StevenDouglas</td>\n",
       "      <td>POSITION SUMMARY:\\nThe Data Scientist provides...</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2080</td>\n",
       "      <td>Experience LLC</td>\n",
       "      <td>When your team hits a game winner or the band ...</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2081</td>\n",
       "      <td>Arby's</td>\n",
       "      <td>Purpose Of The Position\\n\\nAn Inspire data sci...</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2082</td>\n",
       "      <td>Collabera Inc.</td>\n",
       "      <td>Atlanta, Georgia\\nSkills : python ,r ,scala ,j...</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index         company                                        description  \\\n",
       "0   2078  PRI Technology  Sr. Data Scientist\\n\\nThe Sr. Data Scientist i...   \n",
       "1   2079   StevenDouglas  POSITION SUMMARY:\\nThe Data Scientist provides...   \n",
       "2   2080  Experience LLC  When your team hits a game winner or the band ...   \n",
       "3   2081          Arby's  Purpose Of The Position\\n\\nAn Inspire data sci...   \n",
       "4   2082  Collabera Inc.  Atlanta, Georgia\\nSkills : python ,r ,scala ,j...   \n",
       "\n",
       "          job_cat  loc            position  \n",
       "0  data scientist  ATL  Sr. Data Scientist  \n",
       "1  data scientist  ATL      Data Scientist  \n",
       "2  data scientist  ATL      Data Scientist  \n",
       "3  data scientist  ATL      Data Scientist  \n",
       "4  data scientist  ATL      Data Scientist  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = ['NY', 'KCMO', 'ATL']\n",
    "roles = ['data scientist', 'data analyst']\n",
    "# df['loc'].unique()\n",
    "df_filteredbyloc = df[df['loc'].isin(locations)]\n",
    "df_filteredbyloc = df_filteredbyloc[df_filteredbyloc['job_cat'].isin(roles)].reset_index()\n",
    "df_filteredbyloc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA CLEANING - NLP PREP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/blewis2/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec, Doc2Vec, TfidfModel\n",
    "from gensim.models.phrases import Phraser, Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_set = stopwords.words('english')\n",
    "stopword_set = stopword_set + [\"need\", \"goal\", \"include\", \"looking\", \"seeking\"] + list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text, stopwords=stopwords.words('english'), symbols = \"?&()*%$#@.!:;^\"):\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"*\", \"\").strip().strip(\"\\n\").replace(\"-\", \"_\")\n",
    "    for symbol in symbols:\n",
    "        text = text.replace(symbol, \"\")\n",
    "    text = text.lower()\n",
    "    new_text = []\n",
    "    for word in text.split(\" \"):\n",
    "        if word in stopwords or word == ' ' or word == '':\n",
    "            continue\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = df_filteredbyloc.description \n",
    "descriptions = [cleaner(d, stopwords=stopword_set) for d in descriptions]\n",
    "df_filteredbyloc['descriptions'] = descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phrases(sentences = descriptions, min_count=1, threshold=1)\n",
    "phrased_descriptions = [bigram[d] for d in descriptions]\n",
    "df_filteredbyloc['phrased_descriptions'] = phrased_descriptions\n",
    "df_filteredbyloc['cleaned_phrased_descriptions'] = [' '.join(d) for d in df_filteredbyloc.phrased_descriptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_filteredbyloc['job_cat']\n",
    "X = df_filteredbyloc['cleaned_phrased_descriptions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.04009723, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorize data\n",
    "count_vectorizer = CountVectorizer()\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "# count_vectorizer.fit(X.description)\n",
    "# count_vector = count_vectorizer.transform(X.description)\n",
    "# # summarize encoded vector\n",
    "# print(count_vector.shape)\n",
    "# print(type(count_vector))\n",
    "# # print(vector.toarray())\n",
    "# vectorizer.fit_transform(df_filteredbyloc)\n",
    "# vectorizer.vocabulary_\n",
    "X_t = tfidf_vectorizer.fit_transform(X)\n",
    "X_t.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_t.toarray(), y, test_size=0.2,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9o9tRLY-y51t"
   },
   "source": [
    "SCRUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OESdrmQGy7e4"
   },
   "outputs": [],
   "source": [
    "df_filteredbyloc.sum().isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gHMYbgBbzBal"
   },
   "source": [
    "EXPLORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filteredbyloc.groupby('loc').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iTpi55HOzFbt"
   },
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BT3_sTwUzZpq"
   },
   "outputs": [],
   "source": [
    "#library imnports - model\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification model pipelines\n",
    "pipelines = {\n",
    "    'multinomialnb': make_pipeline(TfidfVectorizer(), MultinomialNB()),\n",
    "    'logisticregression': make_pipeline(TfidfVectorizer(), LogisticRegression()),\n",
    "    'randomforestclassifier': make_pipeline(TfidfVectorizer(), RandomForestClassifier()),\n",
    "    'gradientboostingclassifier': make_pipeline(TfidfVectorizer(), GradientBoostingClassifier())\n",
    "}\n",
    "#define hyperparameters for each model \n",
    "multinomialnb_hyperparameters = {\n",
    "    'multinomialnb__alpha': np.linspace(0.5, 1.5, 6),\n",
    "    'multinomialnb__fit_prior': [True, False]\n",
    "}\n",
    "logisticregression_hyperparameters = {\n",
    "    'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "randomforestclassifier_hyperparameters = {\n",
    "    'randomforestclassifier__n_estimators': [100, 200], \n",
    "    'randomforestclassifier__max_features': ['auto', 'sqrt',0.33]\n",
    "}\n",
    "gradientboostingclassifier_hyperparameters = {\n",
    "    'gradientboostingclassifier__n_estimators': [100, 200], \n",
    "    'gradientboostingclassifier__learning_rate': [0.05, 0.1, 0.2],\n",
    "    'gradientboostingclassifier__max_depth': [1, 3, 5]\n",
    "}\n",
    "#set hyperparameter dictionary\n",
    "hyperparameters = {\n",
    "    'multinomialnb': multinomialnb_hyperparameters,\n",
    "    'logisticregression': logisticregression_hyperparameters,\n",
    "    'randomforestclassifier': randomforestclassifier_hyperparameters,\n",
    "    'gradientboostingclassifier': gradientboostingclassifier_hyperparameters\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_models = {}\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    model = GridSearchCV(pipeline, hyperparameters[name], cv=10, n_jobs=-1, scoring='roc_auc')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_hat = model.predict(X_test)\n",
    "    confusion_matrix(y_hat, y_test)\n",
    "    fitted_models[name] = model\n",
    "    print(name, 'has been fitted.')\n",
    "\n",
    "for name, model in fitted_models.items(): \n",
    "    print(name, model.best_score_)\n",
    "    y_hat = model.predict(X_test)\n",
    "    confusion_matrix(y_hat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_features=0.33, random_state=0) \n",
    "model.fit(tfidf_train, y_train)\n",
    "y_hat = model.predict(tfidf_test)\n",
    "print(confusion_matrix(y_hat, y_test))\n",
    "print(classification_report(y_hat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in fitted_models.items(): \n",
    "    print(name, model.best_score_)\n",
    "    y_hat = model.predict(X_test)\n",
    "    print(confusion_matrix(y_hat, y_test))\n",
    "    print(classification_report(y_hat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_models['randomforestclassifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_models['randomforestclassifier'].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save classifier to a file\n",
    "import pickle \n",
    "\n",
    "save_classifier = open(\"Tfidf_randomforestclassifier.pickle\", 'wb') #wb= write in bytes. \n",
    "pickle.dump(fitted_models['randomforestclassifier'], save_classifier) #use pickle to dump the grid3 we trained, as 'Tfidf_LogR.pickle' in wb format\n",
    "save_classifier.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomforestclassifier = fitted_models['randomforestclassifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the saved file and uplaod it to an object\n",
    "\n",
    "model_randomforestclassifier = open(\"Tfidf_randomforestclassifier.pickle\", 'rb') # rb= read in bytes\n",
    "grid = pickle.load(model_randomforestclassifier)\n",
    "model_randomforestclassifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 97   7]\n",
      " [  9 122]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "  data analyst       0.92      0.93      0.92       104\n",
      "data scientist       0.95      0.93      0.94       131\n",
      "\n",
      "     micro avg       0.93      0.93      0.93       235\n",
      "     macro avg       0.93      0.93      0.93       235\n",
      "  weighted avg       0.93      0.93      0.93       235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# non-pipeline rf_classifier for convenience\n",
    "clf = RandomForestClassifier(max_features=0.33, n_estimators=100, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "print(confusion_matrix(y_hat, y_test))\n",
    "print(classification_report(y_hat, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Feature Importances of Each Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data scientist    633\n",
       "data analyst      542\n",
       "Name: job_cat, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = clf.feature_importances_.argsort()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-adfcd779d53d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-adfcd779d53d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    top_10_ds = [clf.feature_importances.argsort()[-10:] for y.value_counts()[0]]\u001b[0m\n\u001b[0m                                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "top_10_ds = [clf.feature_importances.argsort()[-10:] for y.value_counts()[0]]\n",
    "\n",
    "for index in top_10_ds:\n",
    "    print(tfidf_vectorizer.get_feature_names()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "science\n",
      "scientist\n",
      "reporting\n",
      "data_scientists\n",
      "reports\n",
      "learning\n",
      "data_science\n",
      "machine_learning\n",
      "data_analyst\n",
      "data_scientist\n"
     ]
    }
   ],
   "source": [
    "for index in top_10:\n",
    "    print(tfidf_vectorizer.get_feature_names()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.012356849606757031,\n",
       " 0.014717732693692143,\n",
       " 0.015501973843842942,\n",
       " 0.016314041955089564,\n",
       " 0.017472926816099712,\n",
       " 0.020245186524562135,\n",
       " 0.08655514909765775,\n",
       " 0.14632182342387842,\n",
       " 0.17582859591540084,\n",
       " 0.19929635783246208]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(clf.feature_importances_)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importances for entire model\n",
    "first_map = dict(zip(tfidf_vectorizer.get_feature_names(), X_t.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-0fe7af5a75b8>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-0fe7af5a75b8>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    for k, v in sorted(dummy_map.items()):\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dummy_map = dict(zip(sorted(clf.feature_importances_), X_t.toarray()[0])\n",
    "            \n",
    "for k, v in dummy_map.items():\n",
    "    if v > 0:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "team 0.030413982321483155\n",
      "work 0.030594109073129188\n",
      "machine_learning 0.03160622239778968\n",
      "data_analysis 0.03690453107295949\n",
      "python 0.03729677233643706\n",
      "computer_science 0.03735365745910442\n",
      "bachelor 0.03976034134350844\n",
      "engineering 0.04223051784065325\n",
      "data_mining 0.043379725245559615\n",
      "problems 0.04372499673522645\n",
      "client 0.045095897048819875\n",
      "ability_work 0.04548377149646243\n",
      "_python 0.047144629010824717\n",
      "master 0.048285055276612164\n",
      "software 0.050449257475217814\n",
      "work_experience 0.05173609098469197\n",
      "health 0.05266003472407853\n",
      "_sql 0.05469942522400743\n",
      "_machine 0.05771615282167054\n",
      "concepts 0.05771615282167054\n",
      "ai 0.05816868219931637\n",
      "data 0.05834945520066635\n",
      "healthcare 0.05961487543615992\n",
      "languages 0.05961487543615992\n",
      "natural_language 0.05961487543615992\n",
      "statistical_modeling 0.06066318835828573\n",
      "use_data 0.060937183690467446\n",
      "statistical_models 0.06238806452007947\n",
      "_physics 0.06269595845170343\n",
      "experience_preferred 0.06301033483476037\n",
      "high_quality 0.06333147251593664\n",
      "data_scientist 0.06415810649265051\n",
      "understanding_data 0.06541844349151808\n",
      "knowledge_experience 0.06618437171782612\n",
      "applying 0.0678452292321884\n",
      "structures 0.06829100342915292\n",
      "initiative 0.06875049588766233\n",
      "prior_experience 0.06875049588766233\n",
      "years_professional 0.0692245778213347\n",
      "sr_data 0.07185103470614249\n",
      "explore 0.07243669120605566\n",
      "s_degree 0.07369865664759884\n",
      "_also 0.07504019293393477\n",
      "data_exploration 0.07504019293393477\n",
      "_software 0.07653467182850797\n",
      "solving_complex 0.07653467182850797\n",
      "highly_quantitative 0.0773420212371082\n",
      "meet_needs 0.07819552934287025\n",
      "learning 0.07833906975493364\n",
      "develop_algorithms 0.08006450627692287\n",
      "wide_array 0.08006450627692287\n",
      "extracting 0.08109473036989368\n",
      "role_responsible 0.08109473036989368\n",
      "_managing 0.08220133481682433\n",
      "work_large 0.08339655867306713\n",
      "solutions_help 0.08469584097579368\n",
      "solutions_using 0.08469584097579368\n",
      "_extract 0.08611904371288176\n",
      "analyst_support 0.08611904371288176\n",
      "analyzing_complex 0.08611904371288176\n",
      "requirements_minimum 0.08611904371288176\n",
      "working_relational 0.08611904371288176\n",
      "build_scalable 0.08769232134779005\n",
      "develop_deep 0.08769232134779005\n",
      "_query 0.09144503048057552\n",
      "development_processes 0.09144503048057552\n",
      "preferred_5 0.09144503048057552\n",
      "responsible_executing 0.09144503048057552\n",
      "well_working 0.09144503048057552\n",
      "_scripting 0.09374685878374896\n",
      "analytic_findings 0.09374685878374896\n",
      "clinical_outcomes 0.09374685878374896\n",
      "customers_improve 0.09374685878374896\n",
      "driven_insights 0.09374685878374896\n",
      "interpreting_business 0.09374685878374896\n",
      "mastery_statistical 0.09374685878374896\n",
      "opportunities_process 0.09374685878374896\n",
      "presentations_communicate 0.09374685878374896\n",
      "processing_using 0.09374685878374896\n",
      "technical_leader 0.09374685878374896\n",
      "techniques_methodologies 0.09374685878374896\n",
      "administrative_healthcare 0.0964693438235636\n",
      "ai_technologies 0.0964693438235636\n",
      "analytically_minded 0.0964693438235636\n",
      "create_dashboards 0.0964693438235636\n",
      "given_client 0.0964693438235636\n",
      "independently_take 0.0964693438235636\n",
      "patient_outcomes 0.0964693438235636\n",
      "preferred_r 0.0964693438235636\n",
      "processes_design 0.0964693438235636\n",
      "quantitative_analyst 0.0964693438235636\n",
      "techniques_collect 0.0964693438235636\n",
      "technologies_improve 0.0964693438235636\n",
      "_packages_ 0.09980139621970788\n",
      "advocate_data_driven 0.09980139621970788\n",
      "analysis_highly 0.09980139621970788\n",
      "applied_analytics 0.09980139621970788\n",
      "authoring_sql 0.09980139621970788\n",
      "co_operative_team 0.09980139621970788\n",
      "concepts_uncover 0.09980139621970788\n",
      "customers_biggest 0.09980139621970788\n",
      "data_evidence 0.09980139621970788\n",
      "date_sources_solid 0.09980139621970788\n",
      "design_architecture 0.09980139621970788\n",
      "engagements_design 0.09980139621970788\n",
      "familiarity_variety 0.09980139621970788\n",
      "generation_drive 0.09980139621970788\n",
      "health_record 0.09980139621970788\n",
      "innovation_via 0.09980139621970788\n",
      "insights_structured 0.09980139621970788\n",
      "nltk_sql 0.09980139621970788\n",
      "player_proficient 0.09980139621970788\n",
      "problems_build 0.09980139621970788\n",
      "process_produce 0.09980139621970788\n",
      "production_ready_analytics 0.09980139621970788\n",
      "questions_applying 0.09980139621970788\n",
      "reproducible_results 0.09980139621970788\n",
      "robust_technical 0.09980139621970788\n",
      "substantial_experience 0.09980139621970788\n",
      "technically_oriented 0.09980139621970788\n",
      "tool_kit 0.09980139621970788\n",
      "unstructured_electronic 0.09980139621970788\n",
      "field 0.10230086896955758\n",
      "databases 0.10407552881879527\n",
      "analytics_portfolio 0.1040971588944308\n",
      "driven_interest 0.1040971588944308\n",
      "models_sr 0.1040971588944308\n",
      "perform_large 0.1040971588944308\n",
      "steps_modeling 0.1040971588944308\n",
      "successfully_executed 0.1040971588944308\n",
      "applications_apply 0.11015169633038972\n",
      "develop_functionality 0.11015169633038972\n",
      "improvement_responsibilities 0.11015169633038972\n",
      "mathematics_data 0.11015169633038972\n",
      "scale_experimentation 0.11015169633038972\n",
      "scale_real_world 0.11015169633038972\n",
      "scientist_sr 0.11015169633038972\n"
     ]
    }
   ],
   "source": [
    "for k, v in sorted(first_map.items(), key=lambda v: v[1], reverse=False):\n",
    "    if v > 0.00:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-66861270d16c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfirst_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'importance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "first_map.columns = ['word', 'importance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>develop_functionality</th>\n",
       "      <td>0.110152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mathematics_data</th>\n",
       "      <td>0.110152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>improvement_responsibilities</th>\n",
       "      <td>0.110152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scale_experimentation</th>\n",
       "      <td>0.110152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scale_real_world</th>\n",
       "      <td>0.110152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientist_sr</th>\n",
       "      <td>0.110152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>applications_apply</th>\n",
       "      <td>0.110152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>steps_modeling</th>\n",
       "      <td>0.104097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perform_large</th>\n",
       "      <td>0.104097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>driven_interest</th>\n",
       "      <td>0.104097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>models_sr</th>\n",
       "      <td>0.104097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analytics_portfolio</th>\n",
       "      <td>0.104097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>successfully_executed</th>\n",
       "      <td>0.104097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>databases</th>\n",
       "      <td>0.104076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>field</th>\n",
       "      <td>0.102301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>substantial_experience</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_evidence</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insights_structured</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concepts_uncover</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>questions_applying</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_sources_solid</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>design_architecture</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nltk_sql</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>co_operative_team</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engagements_design</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>familiarity_variety</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generation_drive</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robust_technical</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health_record</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>technically_oriented</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customers_biggest</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>innovation_via</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>production_ready_analytics</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>problems_build</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unstructured_electronic</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>process_produce</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advocate_data_driven</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tool_kit</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analysis_highly</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_packages_</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reproducible_results</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>applied_analytics</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>authoring_sql</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>player_proficient</th>\n",
       "      <td>0.099801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patient_outcomes</th>\n",
       "      <td>0.096469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>given_client</th>\n",
       "      <td>0.096469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              importance\n",
       "develop_functionality           0.110152\n",
       "mathematics_data                0.110152\n",
       "improvement_responsibilities    0.110152\n",
       "scale_experimentation           0.110152\n",
       "scale_real_world                0.110152\n",
       "scientist_sr                    0.110152\n",
       "applications_apply              0.110152\n",
       "steps_modeling                  0.104097\n",
       "perform_large                   0.104097\n",
       "driven_interest                 0.104097\n",
       "models_sr                       0.104097\n",
       "analytics_portfolio             0.104097\n",
       "successfully_executed           0.104097\n",
       "databases                       0.104076\n",
       "field                           0.102301\n",
       "substantial_experience          0.099801\n",
       "data_evidence                   0.099801\n",
       "insights_structured             0.099801\n",
       "concepts_uncover                0.099801\n",
       "questions_applying              0.099801\n",
       "date_sources_solid              0.099801\n",
       "design_architecture             0.099801\n",
       "nltk_sql                        0.099801\n",
       "co_operative_team               0.099801\n",
       "engagements_design              0.099801\n",
       "familiarity_variety             0.099801\n",
       "generation_drive                0.099801\n",
       "robust_technical                0.099801\n",
       "health_record                   0.099801\n",
       "technically_oriented            0.099801\n",
       "customers_biggest               0.099801\n",
       "innovation_via                  0.099801\n",
       "production_ready_analytics      0.099801\n",
       "problems_build                  0.099801\n",
       "unstructured_electronic         0.099801\n",
       "process_produce                 0.099801\n",
       "advocate_data_driven            0.099801\n",
       "tool_kit                        0.099801\n",
       "analysis_highly                 0.099801\n",
       "_packages_                      0.099801\n",
       "reproducible_results            0.099801\n",
       "applied_analytics               0.099801\n",
       "authoring_sql                   0.099801\n",
       "player_proficient               0.099801\n",
       "patient_outcomes                0.096469\n",
       "given_client                    0.096469"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_first_map = pd.DataFrame.from_dict(first_map, orient='index')\n",
    "df_first_map.columns = ['importance']\n",
    "df_first_map = df_first_map[df_first_map.importance > 0.00]\n",
    "df_first_map.sort_values(by='importance', ascending=False).head(46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec_ds = \n",
    "# vec_da = \n",
    "ds_map = dict(zip([tfidf_vectorizer.get_feature_names() for i in df_filteredbyloc[df_filteredbyloc['job_cat']=='data scientist'].index], [X_t.toarray()[0] for y in df_filteredbyloc[df_filteredbyloc['job_cat']=='data scientist'].index]))\n",
    "da_map = dict(zip([tfidf_vectorizer.get_feature_names() for i in df_filteredbyloc[df_filteredbyloc['job_cat']=='data analyst'].index], [X_t.toarray()[0] for y in df_filteredbyloc[df_filteredbyloc['job_cat']=='data analyst'].index]))\n",
    "# dict(zip(tfidf_vectorizer.get_feature_names(), X_t.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in ds_map.items():\n",
    "    if v > 0:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in da_map.items():\n",
    "    if v > 0:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importances for ['data scientist']\n",
    "# dspos_map = dict(zip()\n",
    "print(tfidf_vectorizer.dtype)\n",
    "print(X_t.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importances for ['data engineer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 1, (Source: https://buhrmann.github.io/tfidf-analysis.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape\n",
    "# type(X_train)\n",
    "len(X.shape) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Xtr = pipelines['randomforestclassifier'].fit_transform(list(X.values))\n",
    "Xtr = X_t.toarray()\n",
    "vec = clf.named_steps['tfidfvectorizer']\n",
    "features = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_feats_in_doc(Xtr, features, row_id, top_n=25):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_feats_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = []\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)\n",
    "        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs.append(feats_df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tfidf_classfeats_h(dfs):\n",
    "    ''' Plot the data frames returned by the function plot_tfidf_classfeats(). '''\n",
    "    fig = plt.figure(figsize=(12, 9), facecolor=\"w\")\n",
    "    x = np.arange(len(dfs[0]))\n",
    "    for i, df in enumerate(dfs):\n",
    "        ax = fig.add_subplot(1, len(dfs), i+1)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.set_frame_on(False)\n",
    "        ax.get_xaxis().tick_bottom()\n",
    "        ax.get_yaxis().tick_left()\n",
    "        ax.set_xlabel(\"Mean Tf-Idf Score\", labelpad=16, fontsize=14)\n",
    "        ax.set_title(\"label = \" + str(df.label), fontsize=16)\n",
    "        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n",
    "        ax.barh(x, df.tfidf, align='center', color='#3F5D7D')\n",
    "        ax.set_yticks(x)\n",
    "        ax.set_ylim([-1, x[-1]+1])\n",
    "        yticks = ax.set_yticklabels(df.feature)\n",
    "        plt.subplots_adjust(bottom=0.09, right=0.97, left=0.15, top=0.95, wspace=0.52)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tfidf_feats(X[0], features, top_n=25)\n",
    "# top_feats_in_doc(Xtr, features, row_id, top_n=25)\n",
    "# top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25)\n",
    "# top_feats_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25)\n",
    "# plot_tfidf_classfeats_h(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = clf_rf.named_steps.randomforestclassifier.feature_importances_.argsort()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in top_10:\n",
    "    print(tfidf_train.get_feature_names()[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf.named_steps.randomforestclassifier.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.DataFrame(clf_rf.named_steps.randomforestclassifier.feature_importances_, index=X_train.columns, columns=['Score']) #creating a list of top 10 features from RF model\n",
    "feat_importances = feat_importances.sort_values(by='Score',ascending=True) #sorting values\n",
    "feat_importances.plot(kind='barh') #plotting the features in a horizontal bar chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vUxxQ9yizru5"
   },
   "outputs": [],
   "source": [
    "vectorizer = fitted_models.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "# transform the training dataset:\n",
    "X_test_set = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# find maximum value for each of the features over dataset:\n",
    "max_value = X_test_set.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "\n",
    "# get feature names\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "print(\"Features with lowest tfidf:\\n{}\".format(\n",
    "      feature_names[sorted_by_tfidf[:20]]))\n",
    "\n",
    "print(\"\\nFeatures with highest tfidf: \\n{}\".format(\n",
    "      feature_names[sorted_by_tfidf[-20:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "# model_randomforestclassifier.Pipeline.named_steps\n",
    "importances = model.feature_importances_\n",
    "# type(randomforestclassifier)\n",
    "# model2 = SelectFromModel(model, prefit=True)\n",
    "# X_new = model2.transform(tfidf_train)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(tfidf_train.shape[0]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(tfidf_train.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(tfidf_train.shape[1]), indices)\n",
    "plt.xlim([-1, tfidf_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_informative_features(model, text=None, n=20):\n",
    "    # Extract the vectorizer and the classifier from the pipeline\n",
    "    vectorizer = model.named_steps['tfidfvectorizer']\n",
    "    classifier = model.named_steps['randomforestclassifier']\n",
    "\n",
    "    # Check to make sure that we can perform this computation\n",
    "    if not hasattr(classifier, 'coef_'):\n",
    "        raise TypeError(\n",
    "            \"Cannot compute most informative features on {}.\".format(\n",
    "                classifier.__class__.__name__\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if text is not None:\n",
    "        # Compute the coefficients for the text\n",
    "        tvec = model.transform([text]).toarray()\n",
    "    else:\n",
    "        # Otherwise simply use the coefficients\n",
    "        tvec = classifier.coef_\n",
    "\n",
    "    # Zip the feature names with the coefs and sort\n",
    "    coefs = sorted(\n",
    "        zip(tvec[0], vectorizer.get_feature_names()),\n",
    "        key=itemgetter(0), reverse=True\n",
    "    )\n",
    "\n",
    "    # Get the top n and bottom n coef, name pairs\n",
    "    topn  = zip(coefs[:n], coefs[:-(n+1):-1])\n",
    "\n",
    "    # Create the output string to return\n",
    "    output = []\n",
    "\n",
    "    # If text, add the predicted value to the output.\n",
    "    if text is not None:\n",
    "        output.append(\"\\\"{}\\\"\".format(text))\n",
    "        output.append(\n",
    "            \"Classified as: {}\".format(model.predict([text]))\n",
    "        )\n",
    "        output.append(\"\")\n",
    "\n",
    "    # Create two columns with most negative and most positive features.\n",
    "    for (cp, fnp), (cn, fnn) in topn:\n",
    "        output.append(\n",
    "            \"{:0.4f}{: >15}    {:0.4f}{: >15}\".format(\n",
    "                cp, fnp, cn, fnn\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_most_informative_features(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6T4CfSm3zHUK"
   },
   "source": [
    "INTERPRET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sentence = 'he said that she said \"hello\".'\n",
    "pattern = 'he'\n",
    "p = re.compile(pattern)\n",
    "p.findall(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Modeling.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
